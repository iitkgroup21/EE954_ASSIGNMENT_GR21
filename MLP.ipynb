{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/iitkgroup21/EE954_ASSIGNMENT_GR21/blob/anup_changes/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zsArNX56W08"
   },
   "source": [
    "# About Dataset\n",
    "### Context\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\"\n",
    "\n",
    "Zalando seeks to replace the original MNIST dataset\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "* **Image Resolution:** Each image in the MNIST dataset is 28x28 pixels, with a single color channel (grayscale).\n",
    "* **Number of Classes:** The dataset has 10 classes, representing the digits 0 through 9.\n",
    "* **Color Format:** Grayscale (1 channel), with pixel values ranging from 0 to 255 in the raw data. After applying transforms.ToTensor(), these values are scaled between 0 and 1.\n",
    "\n",
    "### Dataset Composition\n",
    "* **Training Set:** 60,000 images, used for training models.\n",
    "* **Test Set:** 10,000 images, used for evaluating model performance.\n",
    "\n",
    "### Typical Usage\n",
    "The dataset is often divided into three subsets for practical machine learning workflows:\n",
    "\n",
    "* **Training Set (90% of the original training data)**: Used for training the model on 54,000 images.\n",
    "* **Validation Set (10% of the original training data):** Used for tuning hyperparameters and preventing overfitting, with 6,000 images.\n",
    "* **Test Set (100% of the original Testing data):** Used for final evaluation, with 10,000 images.\n",
    "\n",
    "#### Labels\n",
    "\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "* 0 T-shirt/top\n",
    "* 1 Trouser\n",
    "* 2 Pullover\n",
    "* 3 Dress\n",
    "* 4 Coat\n",
    "* 5 Sandal\n",
    "* 6 Shirt\n",
    "* 7 Sneaker\n",
    "* 8 Bag\n",
    "* 9 Ankle boot\n",
    "\n",
    "\n",
    "#### Transformation\n",
    "\n",
    "* **ToTensor:** Converts each image to a PyTorch tensor and scales the pixel values to the range [0, 1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeEZavkak4L4"
   },
   "source": [
    "\n",
    "## Basic concepts of CNN model :\n",
    "\n",
    "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm that can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image, and be able to differentiate one from the other.\n",
    "\n",
    "Three basic components to define a basic convolutional neural network.\n",
    "\n",
    "*   The Convolutional Layer\n",
    "*   The Pooling layer\n",
    "*   The Output layer\n",
    "\n",
    "![](https://media.licdn.com/dms/image/v2/D5612AQGOui8XZUZJSA/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1680532048475?e=1735776000&v=beta&t=Evq_XWpAo5JDVF4dy5tw2L8E7KDUgYwDrKtnTi5Go_I)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym3JiZUJp3T2"
   },
   "source": [
    "# Code Explanation\n",
    "\n",
    "## Importing Libraries\n",
    "- **numpy**: Library for numerical operations.\n",
    "- **matplotlib.pyplot**: Library for plotting graphs.\n",
    "- **pandas**: Library for data manipulation.\n",
    "- **wandb**: Library for experiment tracking and model management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8NlkmDDc7gj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8v0iH6tp3T3"
   },
   "source": [
    "## Logging into Weights & Biases\n",
    "- Logs into Weights & Biases for tracking experiments and storing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6nONAFtdcae"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import torch\n",
    "\n",
    "dataset_location = root = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3k_NpFcfp3T3"
   },
   "source": [
    "## Loading and Transforming Dataset\n",
    "- **torchvision.datasets**: Provides easy access to standard datasets.\n",
    "- **torchvision.transforms**: Provides common transformations.\n",
    "- **torch.utils.data**: Utility functions for data loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIfN45bsz0MR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH_20dgBp3T4"
   },
   "source": [
    "## Data Preparation\n",
    "### Loading FashionMNIST Dataset\n",
    "- Loads the FashionMNIST dataset and applies the `ToTensor` transformation to convert images to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_LQ630yjnG1"
   },
   "outputs": [],
   "source": [
    "training_dataset = datasets.FashionMNIST(dataset_location,\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(dataset_location,\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNDK4rqNphbu"
   },
   "outputs": [],
   "source": [
    "print(\"training dataset length =\", len(training_dataset), \"test dataset length =\", len(test_dataset))\n",
    "nc=10 #number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9fkUzkjdsQo"
   },
   "outputs": [],
   "source": [
    "#Initializing the ratios for the test, training and validation datasets\n",
    "train_dataset_ratio = 0.9\n",
    "validation_dataset_ratio = 0.1\n",
    "test_dataset_ratio = 1.0 # This is applied to the full test data set\n",
    "#Initalizing the new values of the training, testing and validation data sizes\n",
    "train_dataset_size = int(train_dataset_ratio * len(training_dataset))\n",
    "test_dataset_size = int(test_dataset_ratio * len(test_dataset))\n",
    "validation_dataset_size = int(validation_dataset_ratio * len(training_dataset))\n",
    "#create the datasets with the sizes\n",
    "new_train_dataset, new_validation_dataset = torch.utils.data.random_split(training_dataset, [train_dataset_size, validation_dataset_size])\n",
    "#new_test_dataset = torch.utils.data.random_split(test_dataset, [test_dataset_size]) # This is a redundant step but will be useful if the ratios change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHjl0HB7oEAX"
   },
   "outputs": [],
   "source": [
    "print(f\"train_dataset_size = {train_dataset_size}, test_dataset_size = {test_dataset_size}, validation_dataset_size = {validation_dataset_size}, new_train_dataset length = {len(new_train_dataset)}, new_validation_dataset length = {len(new_validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEkpYtTjzg4n"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "sample_image, sample_label = new_train_dataset[0]\n",
    "print(f\"Min pixel value: {sample_image.min().item()}, Max pixel value: {sample_image.max().item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UM1UI5Sp3T6"
   },
   "source": [
    "## Model Definition\n",
    "### Custom Convolutional Neural Network\n",
    "- Defines a custom Convolutional Neural Network (CNN) with layers for convolution, batch normalization, activation, and pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6q_RQZuAvXbL"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Defining class names for Fashion MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Displaying a 4x4 grid of the first 16 images in the dataset with pixel size and pixel range\n",
    "plt.figure(figsize=(8, 8))  # Set the size of the figure\n",
    "for i in range(12):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    image, label = new_train_dataset[i]\n",
    "\n",
    "    min_pixel = image.min().item()\n",
    "    max_pixel = image.max().item()\n",
    "\n",
    "    plt.imshow(image.reshape((28,28)).squeeze())\n",
    "    #plt.imshow(image.squeeze())\n",
    "\n",
    "    # Display the class name, pixel size, and pixel range in the title\n",
    "    plt.title(f\"{class_names[label]}\\n28x28 pixels\\nRange: {min_pixel:.2f}-{max_pixel:.2f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "plt.show()  # Show the 6x6 grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLRk7Gk9vb2O"
   },
   "outputs": [],
   "source": [
    "# Now, display a random 15x15 grid of images\n",
    "W_grid = 10\n",
    "L_grid = 10\n",
    "\n",
    "fig, axes = plt.subplots(L_grid, W_grid, figsize = (15,15))\n",
    "axes = axes.ravel() # Flatten the grid to make it easier to access each subplot, 2D to 1D\n",
    "n_train = len(new_train_dataset)\n",
    "\n",
    "for i in np.arange(0, W_grid * L_grid):\n",
    "    index = np.random.randint(0, n_train)\n",
    "    sample_image, sample_label = new_train_dataset[index]\n",
    "    axes[i].imshow(sample_image.reshape((28,28)))\n",
    "    axes[i].set_title(class_names[sample_label], fontsize = 9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wBPPGv_0Ihz"
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "     def __init__(self):\n",
    "       super(CustomModel, self).__init__()\n",
    "       self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "     def forward(self, x):\n",
    "            x = self.conv_layer(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81DeGjmd0QYL"
   },
   "outputs": [],
   "source": [
    "# Initialize DataLoaders to retrieve batches of data\n",
    "train_loader = DataLoader(new_train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(new_validation_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcME3qkEp3T6"
   },
   "source": [
    "## Fully Connected Neural Network\n",
    "### Neural Network Definition\n",
    "- Defines a fully connected neural network with methods for adding layers, displaying parameters, and performing forward passes.\n",
    "# NeuralNetwork Class Documentation\n",
    "\n",
    "The `NeuralNetwork` class implements a fully connected neural network with methods for adding layers, performing feedforward and backpropagation, and calculating accuracy and loss.\n",
    "\n",
    "### Class: `NeuralNetwork`\n",
    "\n",
    "#### Attributes:\n",
    "- **`layers`**: List of tuples, where each tuple contains weights and biases for a layer.\n",
    "- **`activation_functions`**: List of activation functions used for each layer.\n",
    "- **`layer_data`**: List of dictionaries containing information about each layer, such as neuron counts, weight and bias sizes, and parameter totals.\n",
    "- **`activations`**: List of activations for each layer during the feedforward process.\n",
    "\n",
    "---\n",
    "\n",
    "### `__init__()`\n",
    "**Signature**: `def __init__(self)`\n",
    "\n",
    "Initializes an empty neural network instance with lists to hold layers, activation functions, and layer information.\n",
    "\n",
    "---\n",
    "\n",
    "### `addnetwork(input_size, input_layers_config, output_layers_config)`\n",
    "**Signature**: `def addnetwork(self, input_size, input_layers_config, output_layers_config)`\n",
    "\n",
    "Adds layers to the neural network.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `input_size` (int): Number of input neurons.\n",
    "  - `input_layers_config` (list of dict): Configuration for input and hidden layers, where each dictionary specifies:\n",
    "    - `'neurons'`: Number of neurons in the layer.\n",
    "    - `'activation'`: Activation function (e.g., `'relu'`, `'softmax'`).\n",
    "  - `output_layers_config` (list of dict): Configuration for the output layer, formatted similarly to `input_layers_config`.\n",
    "\n",
    "---\n",
    "\n",
    "### `display_parameters()`\n",
    "**Signature**: `def display_parameters(self)`\n",
    "\n",
    "Displays the model parameters as a table for easier inspection of each layer's configuration.\n",
    "\n",
    "- **Returns**:\n",
    "  - `DataFrame`: A Pandas DataFrame summarizing layer information, such as neuron counts, weight and bias sizes, and total parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### `flatten(X)`\n",
    "**Signature**: `def flatten(self, X)`\n",
    "\n",
    "Flattens each input (e.g., image or sample) to a 1D vector.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `X` (`numpy.ndarray`): Input data with dimensions `(batch_size, height, width, channels)`.\n",
    "\n",
    "- **Returns**:\n",
    "  - `numpy.ndarray`: Flattened data with shape `(batch_size, flattened_size)`.\n",
    "\n",
    "---\n",
    "\n",
    "### `relu(x)`\n",
    "**Signature**: `def relu(self, x)`\n",
    "\n",
    "Applies the ReLU (Rectified Linear Unit) activation function.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `x` (`numpy.ndarray`): Input data.\n",
    "\n",
    "- **Returns**:\n",
    "  - `numpy.ndarray`: ReLU-transformed output.\n",
    "\n",
    "---\n",
    "\n",
    "### `relu_derivative(x)`\n",
    "**Signature**: `def relu_derivative(self, x)`\n",
    "\n",
    "Calculates the derivative of the ReLU function, used during backpropagation.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `x` (`numpy.ndarray`): Input data.\n",
    "\n",
    "- **Returns**:\n",
    "  - `numpy.ndarray`: Derivative of ReLU, with 1 for positive inputs and 0 otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "### `softmax(x)`\n",
    "**Signature**: `def softmax(self, x)`\n",
    "\n",
    "Applies the softmax function, converting logits to probabilities.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `x` (`numpy.ndarray`): Input data.\n",
    "\n",
    "- **Returns**:\n",
    "  - `numpy.ndarray`: Probabilities, normalized across the output dimension.\n",
    "\n",
    "---\n",
    "\n",
    "### `feedforward(X)`\n",
    "**Signature**: `def feedforward(self, X)`\n",
    "\n",
    "Conducts a feedforward pass through the network, computing activations for each layer.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `X` (`numpy.ndarray`): Input data.\n",
    "\n",
    "- **Returns**:\n",
    "  - `numpy.ndarray`: Final layer output, representing the network’s predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### `compute_loss(yHat, y)`\n",
    "**Signature**: `def compute_loss(self, yHat, y)`\n",
    "\n",
    "Calculates cross-entropy loss between predictions and actual values.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `yHat` (`numpy.ndarray`): Predicted probabilities (output of softmax).\n",
    "  - `y` (`numpy.ndarray`): True labels (one-hot encoded).\n",
    "\n",
    "- **Returns**:\n",
    "  - `float`: Cross-entropy loss value.\n",
    "\n",
    "---\n",
    "\n",
    "### `backwardpass(X, y, yHat, learning_rate=0.01)`\n",
    "**Signature**: `def backwardpass(self, X, y, yHat, learning_rate=0.01)`\n",
    "\n",
    "Performs backpropagation to compute and apply gradients to weights and biases.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `X` (`numpy.ndarray`): Input data.\n",
    "  - `y` (`numpy.ndarray`): True labels.\n",
    "  - `yHat` (`numpy.ndarray`): Predicted output from feedforward.\n",
    "  - `learning_rate` (float, optional): Learning rate for gradient descent (default is 0.01).\n",
    "\n",
    "---\n",
    "\n",
    "### `calculate_accuracy(yHat, y)`\n",
    "**Signature**: `def calculate_accuracy(self, yHat, y)`\n",
    "\n",
    "Computes the classification accuracy by comparing predictions with true labels.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `yHat` (`numpy.ndarray`): Predicted probabilities.\n",
    "  - `y` (`numpy.ndarray`): True labels.\n",
    "\n",
    "- **Returns**:\n",
    "  - `float`: Classification accuracy, as a decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrL6AH5ZusFa"
   },
   "outputs": [],
   "source": [
    "#Fully connected layer\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "      self.layers = []\n",
    "      self.activation_functions = []\n",
    "      self.layer_data = []\n",
    "      self.activations = []\n",
    "\n",
    "    def addnetwork(self, input_size, input_layers_config, output_layers_config):\n",
    "\n",
    "      #addition the first layer\n",
    "      neurons = input_size  # Same number of neurons as input size\n",
    "      activation = input_layers_config[0]['activation']\n",
    "      new_input_layers_config = []\n",
    "\n",
    "      first_layer_neuron = input_layers_config[0]['neurons']\n",
    "\n",
    "\n",
    "      # Initialize weights and biases for the first layer\n",
    "      weights = np.random.randn(input_size, first_layer_neuron)\n",
    "      bias = np.zeros((1, first_layer_neuron))\n",
    "\n",
    "      # Store layer information\n",
    "      self.layer_data.append({\n",
    "          \"Layer\": \"First Layer\",\n",
    "          \"Input Neurons\": input_size,\n",
    "          \"Output Neurons\": first_layer_neuron,\n",
    "          \"Weights\": weights.size,\n",
    "          \"Biases\": bias.size,\n",
    "          \"Total Parameters\": weights.size + bias.size\n",
    "      })\n",
    "\n",
    "      # Append weights, biases, and activation function to the model\n",
    "      #print(f\"first layer weights = {weights.shape}, first layer bias = {bias.shape}\")\n",
    "      self.layers.append((weights, bias))\n",
    "      self.activation_functions.append(activation)\n",
    "      current_input_size = first_layer_neuron\n",
    "      # Addition of hidden layers\n",
    "      #Initialize the weights and biases for the input layers\n",
    "      for i, layer in enumerate(input_layers_config[1:], start=1):\n",
    "        neurons = layer['neurons']\n",
    "        activation = layer['activation']\n",
    "        # Initialize weights and biases for the current layer\n",
    "        weights = np.random.randn(current_input_size, neurons)  # weight initialization\n",
    "        bias = np.zeros((1, neurons))  # Bias initialization\n",
    "        # Store layer information\n",
    "        self.layer_data.append({\n",
    "          \"Layer\": f\"Hidden Layer {i+1}\",\n",
    "          \"Input Neurons\": current_input_size,\n",
    "          \"Output Neurons\": neurons,\n",
    "          \"Weights\": weights.size,\n",
    "          \"Biases\": bias.size,\n",
    "          \"Total Parameters\": weights.size + bias.size\n",
    "        })\n",
    "        # Append weights, biases, and activation function to the model\n",
    "        self.layers.append((weights, bias))\n",
    "        self.activation_functions.append(activation)\n",
    "        # Update current input size for next layer\n",
    "        current_input_size = neurons\n",
    "      # Addition of output layers\n",
    "      # Initialize weights and biases for the output layer\n",
    "      output_neurons = output_layers_config[0]['neurons']\n",
    "      output_activation = output_layers_config[0]['activation']\n",
    "      output_weights = np.random.randn(current_input_size, output_neurons)\n",
    "      output_bias = np.zeros((1, output_neurons))\n",
    "\n",
    "      self.layer_data.append({\n",
    "          \"Layer\": \"Output Layer\",\n",
    "          \"Input Neurons\": current_input_size,\n",
    "          \"Output Neurons\": output_neurons,\n",
    "          \"Weights\": output_weights.size,\n",
    "          \"Biases\": output_bias.size,\n",
    "          \"Total Parameters\": output_weights.size + output_bias.size\n",
    "        })\n",
    "      # Append output weights, biases, and activation function\n",
    "      self.layers.append((output_weights, output_bias))\n",
    "      self.activation_functions.append(output_activation)\n",
    "\n",
    "    def display_parameters(self):\n",
    "      df = pd.DataFrame(self.layer_data)\n",
    "      return df\n",
    "\n",
    "    def flatten(self, X):\n",
    "      batch_size = X.shape[0]\n",
    "      #Flatten each image/sample to a 1D vector\n",
    "      return X.reshape(batch_size, -1) # output(batch size, flattened size)\n",
    "\n",
    "    # Relu activation function\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # Derivative of Relu activation function\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    #Softmax function\n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)# Softmax activation\n",
    "\n",
    "\n",
    "    def feedforward(self, X):\n",
    "      self.activations = [X]  # Initialize with the input\n",
    "      for i, layer in enumerate(self.layers):\n",
    "        # Extract weights and biases for the current layer\n",
    "        weights, bias = layer\n",
    "        \n",
    "        # Matrix multiplication and bias addition\n",
    "        X = np.dot(X, weights) + bias\n",
    "        # Apply activation function\n",
    "        if self.activation_functions[i] == 'relu':\n",
    "          X = self.relu(X)\n",
    "            #X = np.maximum(0, X)  # Using np.maximum for ReLU activation\n",
    "        elif self.activation_functions[i] == 'softmax':\n",
    "          X = self.softmax(X)\n",
    "        # Store the activation for each layer\n",
    "        self.activations.append(X)\n",
    "      yHat = X\n",
    "      return yHat\n",
    "\n",
    "    def compute_loss(self, yHat, y):\n",
    "      # Cross-entropy loss\n",
    "      m = y.shape[0]\n",
    "      loss = -np.sum(y * np.log(yHat + 1e-9)) / m\n",
    "      return loss\n",
    "\n",
    "\n",
    "    def backwardpass(self, X, y, yHat, learning_rate=0.01):\n",
    "      #print(f\"shape of X: {X.shape}, shape of y: {y.shape}, shape of yHat: {yHat.shape}\")\n",
    "      m = y.shape[0]  # Number of examples in the batch\n",
    "      # Store the derivatives for each layer\n",
    "      gradients = []\n",
    "      # Compute the gradient for the output layer (softmax with cross-entropy loss)\n",
    "      dA = yHat - y  # Gradient of the loss with respect to output (yHat)\n",
    "      for i in reversed(range(len(self.layers))):\n",
    "          weights, bias = self.layers[i]\n",
    "          activation = self.activation_functions[i]\n",
    "          # Use the stored activation as the input to this layer\n",
    "          A_prev = self.activations[i]\n",
    "          # Calculate gradients with respect to weights, biases, and inputs for each layer\n",
    "          if activation == 'softmax':\n",
    "              dZ = dA  # dZ for softmax layer\n",
    "          elif activation == 'relu':\n",
    "              dZ = dA * self.relu_derivative(self.activations[i + 1])\n",
    "          # Calculate gradients for weights and biases\n",
    "          dW = np.dot(A_prev.T, dZ) / m\n",
    "          db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "          # Update the weights and biases\n",
    "          weights -= learning_rate * dW\n",
    "          bias -= learning_rate * db\n",
    "          # Update the layer in the network with the new weights and biases\n",
    "          self.layers[i] = (weights, bias)\n",
    "          # Update dA for the next layer in the backpropagation process\n",
    "          dA = np.dot(dZ, weights.T)\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, yHat, y):\n",
    "        # Calculate accuracy based on predictions and true labels\n",
    "        pred_classes = np.argmax(yHat, axis=1)\n",
    "        true_classes = np.argmax(y, axis=1)\n",
    "        return np.mean(pred_classes == true_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kMrxlMzzgtB"
   },
   "outputs": [],
   "source": [
    "input_layers_config = [\n",
    "    {'neurons': 128, 'activation': 'relu'},\n",
    "    #{'neurons': 64, 'activation': 'relu'},\n",
    "    #{'neurons': 32, 'activation': 'relu'}\n",
    "]\n",
    "output_layers_config = [\n",
    "    {'neurons': 10, 'activation': 'softmax'}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bsMLISt7UKf"
   },
   "outputs": [],
   "source": [
    "# Moving model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Epn3sQaBp3T8"
   },
   "source": [
    "### Neural Network Initialization and Layer Addition\n",
    "\n",
    "In this part of the notebook, we initialize our custom model and neural network, and then add layers to the neural network outside the batch loop. Here's a step-by-step explanation of the code:\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   ```python\n",
    "   model = CustomModel().to(device)\n",
    "   nn_network = NeuralNetwork()\n",
    "   ```\n",
    "   - `CustomModel` is instantiated and moved to the specified device (e.g., CPU or GPU).\n",
    "   - `NeuralNetwork` is instantiated but not yet populated with layers.\n",
    "\n",
    "2. **Determining the Shape for Layer Addition**:\n",
    "   ```python\n",
    "   cnn_output_sample = next(iter(train_loader))[0]  # Get a sample batch to determine the shape\n",
    "   ```\n",
    "   - A sample batch is retrieved from `train_loader` to determine the shape of the output feature map from the custom model.\n",
    "\n",
    "3. **Forward Pass Through the Custom Model**:\n",
    "   ```python\n",
    "   cnn_output_sample = model(cnn_output_sample.to(device).type(torch.float32))\n",
    "   ```\n",
    "   - The sample batch is passed through the custom model to get the output feature map. The input is moved to the device and converted to `torch.float32`.\n",
    "\n",
    "4. **Conversion to NumPy Array**:\n",
    "   ```python\n",
    "   cnn_output_np_sample = cnn_output_sample.cpu().detach().numpy()\n",
    "   ```\n",
    "   - The output feature map is moved back to the CPU, detached from the computational graph, and converted to a NumPy array.\n",
    "\n",
    "5. **Flattening the Output**:\n",
    "   ```python\n",
    "   cnn_output_np_sample = nn_network.flatten(cnn_output_np_sample)\n",
    "   ```\n",
    "   - The output feature map is flattened using a method from the `NeuralNetwork` class, preparing it for further processing or layer addition.\n",
    "\n",
    "By performing these steps outside the batch loop, we ensure that the layer additions and shape calculations are done efficiently, avoiding redundant computations during each batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmYMR4J4p3T8"
   },
   "outputs": [],
   "source": [
    "model = CustomModel().to(device)\n",
    "nn_network = NeuralNetwork()\n",
    "# Adding layers to the neural network outside the batch loop\n",
    "cnn_output_sample = next(iter(train_loader))[0]  # Get a sample batch to determine the shape\n",
    "cnn_output_sample = model(cnn_output_sample.to(device).type(torch.float32))\n",
    "cnn_output_np_sample = cnn_output_sample.cpu().detach().numpy()\n",
    "cnn_output_np_sample = nn_network.flatten(cnn_output_np_sample)\n",
    "\n",
    "# Adding layers to the neural network\n",
    "nn_network.addnetwork(input_size=cnn_output_np_sample.shape[1], input_layers_config=input_layers_config, output_layers_config=output_layers_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FugyC4OVp3T8"
   },
   "source": [
    "### Adding Layers to the Neural Network and Initializing Weights & Biases\n",
    "\n",
    "In this section, we add layers to our neural network and initialize Weights & Biases for experiment tracking. Here's a detailed explanation of the code:\n",
    "\n",
    "1. **Adding Layers to the Neural Network**:\n",
    "   - The `addnetwork` method of `nn_network` is called to add layers to the neural network.\n",
    "   - `input_size` is set to the second dimension of the flattened output sample (`cnn_output_np_sample.shape[1]`), which defines the size of the input layer.\n",
    "   - `input_layers_config` and `output_layers_config` are configurations for the input and output layers, respectively.\n",
    "\n",
    "\n",
    "By following these steps, we ensure that our neural network is properly configured and that our experiments are tracked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hCWC_fNp3T9"
   },
   "source": [
    "### Training Hyperparameters and Training Loop\n",
    "\n",
    "In this section, we define the training hyperparameters and implement the training loop for the neural network.\n",
    "\n",
    "1. **Setting Training Hyperparameters**:\n",
    "   - The learning rate and the number of epochs are retrieved configuration.\n",
    "\n",
    "2. **Training Loop**:\n",
    "   - **Epoch Loop**: Iterates over the specified number of epochs.\n",
    "   - **Batch Loop**: Iterates over each batch in the `train_loader`.\n",
    "     - **Data Preparation**: Moves input and label data to the selected device and converts labels to one-hot encoding.\n",
    "     - **Forward Pass**: The input batch is passed through the CNN model, and the output is flattened and fed into the fully connected neural network.\n",
    "     - **Loss Calculation**: Computes the cross-entropy loss between the predicted and true labels.\n",
    "     - **Accuracy Calculation**: Calculates the accuracy of the predictions.\n",
    "     - **Backpropagation**: Performs backpropagation to update the model weights and biases.\n",
    "     - **Epoch Metrics**: Calculates and logs the epoch-level loss\n",
    "\n",
    "3. **Display Parameters**:\n",
    "   - Retrieves and displays the neural network parameter DataFrame to summarize the network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_uE1VQNUK6B"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_one_epoch(model, train_loader, nn_network, device, nc, learning_rate):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_train_loss = 0\n",
    "    correct_train_preds = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Move input data to the selected device\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Prepare labels and input\n",
    "        y_train = F.one_hot(y_batch, num_classes=nc).type(torch.float32)\n",
    "        x_batch = x_batch.type(torch.float32)\n",
    "\n",
    "        # Forward pass through CNN\n",
    "        cnn_output = model(x_batch)\n",
    "        cnn_output_np = cnn_output.cpu().detach().numpy()\n",
    "        cnn_output_np = nn_network.flatten(cnn_output_np)\n",
    "\n",
    "        # Feedforward pass through fully connected network\n",
    "        yhat = nn_network.feedforward(cnn_output_np)\n",
    "\n",
    "        # Compute loss\n",
    "        y_train_np = y_train.cpu().detach().numpy()\n",
    "        loss = nn_network.compute_loss(yhat, y_train_np)\n",
    "        total_train_loss += loss\n",
    "\n",
    "        # Backpropagation\n",
    "        nn_network.backwardpass(cnn_output_np, y_train_np, yhat, learning_rate)\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        preds = np.argmax(yhat, axis=1)\n",
    "        correct_train_preds += np.sum(preds == np.argmax(y_train_np, axis=1))\n",
    "        total_train_samples += y_batch.size(0)\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = correct_train_preds / total_train_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, nn_network, device, nc):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    correct_val_preds = 0\n",
    "    total_val_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for x_val_batch, y_val_batch in val_loader:\n",
    "            x_val_batch = x_val_batch.to(device)\n",
    "            y_val_batch = y_val_batch.to(device)\n",
    "\n",
    "            # Prepare labels and input\n",
    "            y_val = F.one_hot(y_val_batch, num_classes=nc).type(torch.float32)\n",
    "\n",
    "            # Forward pass through CNN and fully connected network\n",
    "            cnn_output_val = model(x_val_batch)\n",
    "            cnn_output_np_val = cnn_output_val.cpu().detach().numpy()\n",
    "            cnn_output_np_val = nn_network.flatten(cnn_output_np_val)\n",
    "            yhat_val = nn_network.feedforward(cnn_output_np_val)\n",
    "\n",
    "            # Compute validation loss\n",
    "            y_val_np = y_val.cpu().detach().numpy()\n",
    "            val_loss = nn_network.compute_loss(yhat_val, y_val_np)\n",
    "            total_val_loss += val_loss\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            val_preds = np.argmax(yhat_val, axis=1)\n",
    "            correct_val_preds += np.sum(val_preds == np.argmax(y_val_np, axis=1))\n",
    "            total_val_samples += y_val_batch.size(0)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = correct_val_preds / total_val_samples\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, nn_network, device, nc, learning_rate, epochs):\n",
    "    train_accuracies, train_losses, val_accuracies, val_losses = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Train the model for one epoch\n",
    "        train_loss, train_accuracy = train_one_epoch(model, train_loader, nn_network, device, nc, learning_rate)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        print(f\"Train loss: {train_loss:.4f}, Train accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = validate(model, val_loader, nn_network, device, nc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Val loss: {val_loss:.4f}, Val accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    return train_accuracies, train_losses, val_accuracies, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies, train_losses, val_accuracies, val_losses = train_and_validate(\n",
    "    model, train_loader, val_loader, nn_network, device, nc, learning_rate=0.0001, epochs=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjCmUDyDwv8S"
   },
   "outputs": [],
   "source": [
    "# Retrieve and display the parameter DataFrame\n",
    "df = nn_network.display_parameters()\n",
    "print(\"Parameter Table for Neural Network:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkwOpeUOp3T9"
   },
   "source": [
    "### Model Evaluation Section\n",
    "\n",
    "- **Evaluation Mode**: The model is set to evaluation mode using `model.eval()`.\n",
    "- **Class Names**: Defined a list of class names for classification.\n",
    "- **Predictions and Labels**: Initialized empty lists to store predictions and true labels.\n",
    "- **Gradient Calculations**: Disabled gradient calculations for efficient evaluation using `torch.no_grad()`.\n",
    "- **Data Loading**: Loaded test data and moved it to the selected device.\n",
    "- **Forward Passes**:\n",
    "  - Passed data through the CNN and converted the output to a numpy array.\n",
    "  - Passed the numpy array through the fully connected network to get predictions.\n",
    "- **Accuracy Calculation**: Calculated accuracy by comparing predictions with true labels.\n",
    "- **Results**:\n",
    "  - Printed the test accuracy.\n",
    "  - Generated and printed a classification report using `classification_report` from `sklearn`.\n",
    "  - Created a confusion matrix and converted it to a DataFrame for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8lI95KDnxBd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "def evaluate_model(model, test_loader, device, nn_network, class_names):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x_test, y_test in test_loader:\n",
    "            # Move data to the selected device\n",
    "            x_test = x_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "\n",
    "            # Forward pass through CNN\n",
    "            cnn_output_test = model(x_test.type(torch.float32))\n",
    "\n",
    "            # Convert to numpy and flatten if needed\n",
    "            cnn_output_test_np = nn_network.flatten(cnn_output_test.cpu().detach().numpy())\n",
    "\n",
    "            # Forward pass through fully connected network\n",
    "            y_pred = nn_network.feedforward(cnn_output_test_np)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = np.argmax(y_pred, axis=1)\n",
    "            correct += (predicted == y_test.cpu().numpy()).sum()\n",
    "            total += y_test.size(0)\n",
    "\n",
    "            all_preds.extend(predicted)\n",
    "            all_labels.extend(y_test.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy score is: {accuracy:.2f}%\")\n",
    "    \n",
    "    return accuracy, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, all_preds, all_labels = evaluate_model(model, test_loader, device, nn_network, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epiYnxbtw-lr"
   },
   "outputs": [],
   "source": [
    "# Calculate and print confusion matrix and classification report\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(all_labels, all_preds),index=class_names, columns=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgbKAyRkt1b_"
   },
   "outputs": [],
   "source": [
    "summary(model, input_size=(1, 28, 28))\n",
    "print(\"Output shape:\", cnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k026XbhexBTH"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Training and Validation Loss vs. Epoch\n",
    "#plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss vs. Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Training and Validation Accuracy vs. Epoch\n",
    "#plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy vs. Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model_configuration(param_grid, model, train_loader, val_loader, nn_network, device, nc):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "\n",
    "    for params in param_grid:\n",
    "        print(f\"Testing configuration: {params}\")\n",
    "        network = NeuralNetwork()\n",
    "        network.addnetwork(input_size=cnn_output_np_sample.shape[1], input_layers_config=input_layers_config, output_layers_config=output_layers_config)\n",
    "        train_accuracies, train_losses, val_accuracies, val_losses = train_and_validate(\n",
    "            model, train_loader, val_loader, network, device, nc, learning_rate=params['learning_rate'], epochs=params['epochs'])\n",
    "        # Get the best validation accuracy for the current parameter configuration\n",
    "        max_val_accuracy = max(val_accuracies)\n",
    "        \n",
    "        # Update the best accuracy and parameters if the current model is better\n",
    "        if max_val_accuracy > best_accuracy:\n",
    "            best_accuracy = max_val_accuracy\n",
    "            best_params = params\n",
    "    return best_params, best_accuracy\n",
    "\n",
    "# Example usage:\n",
    "param_grid = [\n",
    "    {'epochs': 1, 'learning_rate': 0.01},\n",
    "    {'epochs': 2, 'learning_rate': 0.001},\n",
    "    {'epochs': 3, 'learning_rate': 0.005}\n",
    "]\n",
    "\n",
    "best_params, best_accuracy = find_best_model_configuration(param_grid, model, train_loader, val_loader, nn_network, device, nc)\n",
    "print(f\"Best Configuration: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
