{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iitkgroup21/EE954_ASSIGNMENT_GR21/blob/DP/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zsArNX56W08"
      },
      "source": [
        "# About Dataset\n",
        "### Context\n",
        "\n",
        "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
        "\n",
        "The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\"\n",
        "\n",
        "Zalando seeks to replace the original MNIST dataset\n",
        "\n",
        "### Key Characteristics\n",
        "\n",
        "* **Image Resolution:** Each image in the MNIST dataset is 28x28 pixels, with a single color channel (grayscale).\n",
        "* **Number of Classes:** The dataset has 10 classes, representing the digits 0 through 9.\n",
        "* **Color Format:** Grayscale (1 channel), with pixel values ranging from 0 to 255 in the raw data. After applying transforms.ToTensor(), these values are scaled between 0 and 1.\n",
        "\n",
        "### Dataset Composition\n",
        "* **Training Set:** 60,000 images, used for training models.\n",
        "* **Test Set:** 10,000 images, used for evaluating model performance.\n",
        "\n",
        "### Typical Usage\n",
        "The dataset is often divided into three subsets for practical machine learning workflows:\n",
        "\n",
        "* **Training Set (90% of the original training data)**: Used for training the model on 54,000 images.\n",
        "* **Validation Set (10% of the original training data):** Used for tuning hyperparameters and preventing overfitting, with 6,000 images.\n",
        "* **Test Set (100% of the original Testing data):** Used for final evaluation, with 10,000 images.\n",
        "\n",
        "#### Labels\n",
        "\n",
        "Each training and test example is assigned to one of the following labels:\n",
        "\n",
        "* 0 T-shirt/top\n",
        "* 1 Trouser\n",
        "* 2 Pullover\n",
        "* 3 Dress\n",
        "* 4 Coat\n",
        "* 5 Sandal\n",
        "* 6 Shirt\n",
        "* 7 Sneaker\n",
        "* 8 Bag\n",
        "* 9 Ankle boot\n",
        "\n",
        "\n",
        "#### Transformation\n",
        "\n",
        "* **ToTensor:** Converts each image to a PyTorch tensor and scales the pixel values to the range [0, 1].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeEZavkak4L4"
      },
      "source": [
        "\n",
        "## Basic concepts of CNN model :\n",
        "\n",
        "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm that can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image, and be able to differentiate one from the other.\n",
        "\n",
        "Three basic components to define a basic convolutional neural network.\n",
        "\n",
        "*   The Convolutional Layer\n",
        "*   The Pooling layer\n",
        "*   The Output layer\n",
        "\n",
        "![](https://media.licdn.com/dms/image/v2/D5612AQGOui8XZUZJSA/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1680532048475?e=1735776000&v=beta&t=Evq_XWpAo5JDVF4dy5tw2L8E7KDUgYwDrKtnTi5Go_I)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8NlkmDDc7gj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6nONAFtdcae"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "import torch\n",
        "\n",
        "dataset_location = root = './data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIfN45bsz0MR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "m_LQ630yjnG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f086d607-f7fe-47c2-e6db-96b87be94475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54000\n",
            "10000\n",
            "6000\n",
            "54000\n",
            "10000\n",
            "6000\n",
            "Min pixel value: 0.0\n",
            "Max pixel value: 1.0\n"
          ]
        }
      ],
      "source": [
        "training_dataset = datasets.FashionMNIST(dataset_location,\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(dataset_location,\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor(),\n",
        "                              download=True)\n",
        "\n",
        "\n",
        "#Initializing the ratios for the test, training and validation datasets\n",
        "train_dataset_ratio = 0.9\n",
        "validation_dataset_ratio = 0.1\n",
        "test_dataset_ratio = 1.0 # This is applied to the full test data set\n",
        "\n",
        "#Initalizing the new values of the training, testing and validation data sizes\n",
        "train_dataset_size = int(train_dataset_ratio * len(training_dataset))\n",
        "test_dataset_size = int(test_dataset_ratio * len(test_dataset))\n",
        "validation_dataset_size = int(validation_dataset_ratio * len(training_dataset))\n",
        "\n",
        "#create the datasets with the sizes\n",
        "new_train_dataset, new_validation_dataset = torch.utils.data.random_split(training_dataset, [train_dataset_size, validation_dataset_size])\n",
        "new_test_dataset = test_dataset # will need this step for the future if we need to combine other data sets for now, we are using the training data set as it is\n",
        "\n",
        "print(train_dataset_size)\n",
        "print(test_dataset_size)\n",
        "print(validation_dataset_size)\n",
        "print(len(new_train_dataset))\n",
        "print(len(new_test_dataset))\n",
        "print(len(new_validation_dataset))\n",
        "\n",
        "# @title\n",
        "sample_image, sample_label = new_train_dataset[0]\n",
        "print(\"Min pixel value:\", sample_image.min().item())\n",
        "print(\"Max pixel value:\", sample_image.max().item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LbEaWE0JAxOu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "4bf88dd8-3455-40e1-91b2-25e0f8b096b4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title\n",
        "\n",
        "#Defining class names for Fashion MNIST\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Displaying a 4x4 grid of the first 16 images in the dataset with pixel size and pixel range\n",
        "plt.figure(figsize=(8, 8))  # Set the size of the figure\n",
        "for i in range(12):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    image, label = new_train_dataset[i]\n",
        "\n",
        "    min_pixel = image.min().item()\n",
        "    max_pixel = image.max().item()\n",
        "\n",
        "    plt.imshow(image.reshape((28,28)).squeeze())\n",
        "    #plt.imshow(image.squeeze())\n",
        "\n",
        "    # Display the class name, pixel size, and pixel range in the title\n",
        "    plt.title(f\"{class_names[label]}\\n28x28 pixels\\nRange: {min_pixel:.2f}-{max_pixel:.2f}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(hspace=0.6)\n",
        "plt.show()  # Show the 6x6 grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nev0dRTjieuA"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Now, display a random 15x15 grid of images\n",
        "W_grid = 10\n",
        "L_grid = 10\n",
        "\n",
        "fig, axes = plt.subplots(L_grid, W_grid, figsize = (15,15))\n",
        "axes = axes.ravel() # Flatten the grid to make it easier to access each subplot, 2D to 1D\n",
        "n_train = len(new_train_dataset)\n",
        "\n",
        "for i in np.arange(0, W_grid * L_grid):\n",
        "    index = np.random.randint(0, n_train)\n",
        "    sample_image, sample_label = new_train_dataset[index]\n",
        "    axes[i].imshow(sample_image.reshape((28,28)))\n",
        "    axes[i].set_title(class_names[sample_label], fontsize = 9)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.subplots_adjust(hspace=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wBPPGv_0Ihz"
      },
      "outputs": [],
      "source": [
        "class CustomModel(nn.Module):\n",
        "     def __init__(self):\n",
        "       super(CustomModel, self).__init__()\n",
        "       self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(num_features=16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "     def forward(self, x):\n",
        "            x = self.conv_layer(x)\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81DeGjmd0QYL",
        "outputId": "d4551c65-2711-45a1-8fea-8085b5f726d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: torch.Size([64, 1, 28, 28])\n",
            "y_train shape: torch.Size([64])\n",
            "x_val shape: torch.Size([64, 1, 28, 28])\n",
            "y_val shape: torch.Size([64])\n",
            "x_test shape: torch.Size([64, 1, 28, 28])\n",
            "y_test shape: torch.Size([64])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             160\n",
            "       BatchNorm2d-2           [-1, 16, 28, 28]              32\n",
            "              ReLU-3           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-4           [-1, 16, 14, 14]               0\n",
            "            Conv2d-5           [-1, 32, 14, 14]           4,640\n",
            "       BatchNorm2d-6           [-1, 32, 14, 14]              64\n",
            "              ReLU-7           [-1, 32, 14, 14]               0\n",
            "         MaxPool2d-8             [-1, 32, 7, 7]               0\n",
            "            Conv2d-9             [-1, 64, 7, 7]          18,496\n",
            "      BatchNorm2d-10             [-1, 64, 7, 7]             128\n",
            "             ReLU-11             [-1, 64, 7, 7]               0\n",
            "        MaxPool2d-12             [-1, 64, 3, 3]               0\n",
            "           Conv2d-13            [-1, 128, 3, 3]          73,856\n",
            "      BatchNorm2d-14            [-1, 128, 3, 3]             256\n",
            "             ReLU-15            [-1, 128, 3, 3]               0\n",
            "           Conv2d-16            [-1, 256, 3, 3]         295,168\n",
            "      BatchNorm2d-17            [-1, 256, 3, 3]             512\n",
            "             ReLU-18            [-1, 256, 3, 3]               0\n",
            "================================================================\n",
            "Total params: 393,312\n",
            "Trainable params: 393,312\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.62\n",
            "Params size (MB): 1.50\n",
            "Estimated Total Size (MB): 2.13\n",
            "----------------------------------------------------------------\n",
            "Output shape: torch.Size([64, 256, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "model = CustomModel()\n",
        "# Initialize DataLoaders to retrieve batches of data\n",
        "train_loader = DataLoader(new_train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(new_validation_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "# Extract all images and labels from the train and validation sets\n",
        "\n",
        "X_train, y_train = next(iter(train_loader))\n",
        "X_val, y_val = next(iter(val_loader))\n",
        "\n",
        "# Extract all images and labels from the test set\n",
        "X_test, y_test = next(iter(test_loader))\n",
        "\n",
        "\n",
        "print(\"x_train shape:\", X_train.shape)  # Expected shape: (train_size, 1, 28, 28)\n",
        "print(\"y_train shape:\", y_train.shape)  # Expected shape: (train_size)\n",
        "print(\"x_val shape:\", X_val.shape)      # Expected shape: (val_size, 1, 28, 28)\n",
        "print(\"y_val shape:\", y_val.shape)      # Expected shape: (val_size)\n",
        "print(\"x_test shape:\", X_test.shape)      # Expected shape: (val_size, 1, 28, 28)\n",
        "print(\"y_test shape:\", y_test.shape)      # Expected shape: (val_size)\n",
        "\n",
        "\n",
        "# Now, during training, you can iterate over the DataLoader\n",
        "for x_batch, y_batch in train_loader:\n",
        "    # Perform forward pass with x_batch\n",
        "    cnn_output = model(x_batch)\n",
        "\n",
        "summary(model, input_size=(1, 28, 28))\n",
        "print(\"Output shape:\", cnn_output.shape)\n",
        "\n",
        "X_train = X_train.type(torch.float32)\n",
        "X_test = X_test.type(torch.float32)\n",
        "X_val = X_val.type(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GrL6AH5ZusFa"
      },
      "outputs": [],
      "source": [
        "#Fully connected layer\n",
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "      self.layers = []\n",
        "      self.activation_functions = []\n",
        "      self.layer_data = []\n",
        "      self.activations = []\n",
        "\n",
        "    def addnetwork(self, input_size, input_layers_config, output_layers_config):\n",
        "\n",
        "      \"\"\"\n",
        "        - Add the first fully connected layer with the same shape as the flattened CNN output.\n",
        "        - input_size (int): The size of the input layer.\\n\",\n",
        "        - input_layers_config (list): A list of dictionaries specifying the configuration of input layers,\n",
        "        - output_layers_config (list): A list of dictionaries specifying the configuration of output layers.\n",
        "        - layers (list): A list to store the layers of the neural network.\\n\",\n",
        "        - activation_functions (list): A list to store the activation functions for each layer.\\n\",\n",
        "\n",
        "        Args:\n",
        "        - input_size (int): The size of the input layer.\n",
        "        - input_layers_config (list): A list of dictionaries specifying the configuration of input layers.\n",
        "        - output_layers_config (list): A list of dictionaries specifying the configuration of output layers.\n",
        "\n",
        "        Returns:\n",
        "        - None\n",
        "\n",
        "      \"\"\"\n",
        "      #addition the first layer\n",
        "      neurons = input_size  # Same number of neurons as input size\n",
        "      activation = input_layers_config[0]['activation']\n",
        "      new_input_layers_config = []\n",
        "\n",
        "      first_layer_neuron = input_layers_config[0]['neurons']\n",
        "\n",
        "\n",
        "      # Initialize weights and biases for the first layer\n",
        "      weights = np.random.randn(input_size, first_layer_neuron)\n",
        "      bias = np.zeros((1, first_layer_neuron))\n",
        "\n",
        "      # Store layer information\n",
        "      self.layer_data.append({\n",
        "          \"Layer\": \"First Layer\",\n",
        "          \"Input Neurons\": input_size,\n",
        "          \"Output Neurons\": first_layer_neuron,\n",
        "          \"Weights\": weights.size,\n",
        "          \"Biases\": bias.size,\n",
        "          \"Total Parameters\": weights.size + bias.size\n",
        "      })\n",
        "\n",
        "      # Append weights, biases, and activation function to the model\n",
        "      print(\"first layer weights =\", weights.shape)\n",
        "      print(\"first layer bias =\", bias.shape)\n",
        "      self.layers.append((weights, bias))\n",
        "      self.activation_functions.append(activation)\n",
        "\n",
        "\n",
        "      current_input_size = first_layer_neuron\n",
        "\n",
        "      # Addition of hidden layers\n",
        "      #Initialize the weights and biases for the input layers\n",
        "      for i, layer in enumerate(input_layers_config[1:], start=1):\n",
        "        neurons = layer['neurons']\n",
        "        activation = layer['activation']\n",
        "\n",
        "        # Initialize weights and biases for the current layer\n",
        "        weights = np.random.randn(current_input_size, neurons)   # Weight initialization\n",
        "        bias = np.zeros((1, neurons))  # Bias initialization\n",
        "\n",
        "        # Store layer information\n",
        "        self.layer_data.append({\n",
        "          \"Layer\": f\"Hidden Layer {i+1}\",\n",
        "          \"Input Neurons\": current_input_size,\n",
        "          \"Output Neurons\": neurons,\n",
        "          \"Weights\": weights.size,\n",
        "          \"Biases\": bias.size,\n",
        "          \"Total Parameters\": weights.size + bias.size\n",
        "        })\n",
        "\n",
        "        # Append weights, biases, and activation function to the model\n",
        "        self.layers.append((weights, bias))\n",
        "        self.activation_functions.append(activation)\n",
        "\n",
        "        # Update current input size for next layer\n",
        "        current_input_size = neurons\n",
        "\n",
        "      # Addition of output layers\n",
        "      # Initialize weights and biases for the output layer\n",
        "      output_neurons = output_layers_config[0]['neurons']\n",
        "      output_activation = output_layers_config[0]['activation']\n",
        "\n",
        "\n",
        "      output_weights = np.random.randn(current_input_size, output_neurons)\n",
        "      output_bias = np.zeros((1, output_neurons))\n",
        "\n",
        "      self.layer_data.append({\n",
        "          \"Layer\": \"Output Layer\",\n",
        "          \"Input Neurons\": current_input_size,\n",
        "          \"Output Neurons\": output_neurons,\n",
        "          \"Weights\": output_weights.size,\n",
        "          \"Biases\": output_bias.size,\n",
        "          \"Total Parameters\": output_weights.size + output_bias.size\n",
        "        })\n",
        "\n",
        "      # Append output weights, biases, and activation function\n",
        "      self.layers.append((output_weights, output_bias))\n",
        "      self.activation_functions.append(output_activation)\n",
        "\n",
        "    def display_parameters(self):\n",
        "\n",
        "      \"\"\"\n",
        "      Return the parameters of each layer in a DataFrame format.\n",
        "\n",
        "      Returns:\n",
        "      - DataFrame: A pandas DataFrame containing parameter details for each layer.\n",
        "      \"\"\"\n",
        "      df = pd.DataFrame(self.layer_data)\n",
        "      return df\n",
        "\n",
        "    def flatten(self, X):\n",
        "      \"\"\"\n",
        "      Flatten the input data X.\n",
        "\n",
        "      Args:\n",
        "      - X (numpy.ndarray): The input data to be flattened.\n",
        "\n",
        "      Returns:\n",
        "      - numpy.ndarray: The flattened data.\n",
        "      \"\"\"\n",
        "\n",
        "      batch_size = X.shape[0]\n",
        "\n",
        "      #Flatten each image/sample to a 1D vector\n",
        "      return X.reshape(batch_size, -1) # output(batch size, flattened size)\n",
        "\n",
        "    # Relu activation function\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    # Derivative of Relu activation function\n",
        "    def relu_derivative(self, x):\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "    #Softmax function\n",
        "    def softmax(self, x):\n",
        "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)# Softmax activation\n",
        "\n",
        "\n",
        "    def feedforward(self, X):\n",
        "      \"\"\"\n",
        "      Perform a forward pass through the neural network.\n",
        "\n",
        "      Args:\n",
        "      - X (numpy.ndarray): The input to the network from CNN output.\n",
        "\n",
        "      Returns:\n",
        "      - yHat (numpy.ndarray): The output of the fully connected network.\n",
        "\n",
        "      \"\"\"\n",
        "      self.activations = [X]  # Initialize with the input\n",
        "      for i, layer in enumerate(self.layers):\n",
        "        # Extract weights and biases for the current layer\n",
        "        weights, bias = layer\n",
        "\n",
        "        # Debugging shape of X and weights\n",
        "        print(f\"Layer {i + 1}:\")\n",
        "        print(f\"  Input X shape: {X.shape}\")\n",
        "        print(f\"  Weights shape: {weights.shape}\")\n",
        "        print(f\"  Bias shape: {bias.shape}\")\n",
        "\n",
        "        # Matrix multiplication and bias addition\n",
        "        X = np.dot(X, weights) + bias\n",
        "\n",
        "        # Apply activation function\n",
        "        if self.activation_functions[i] == 'relu':\n",
        "          X = self.relu(X)\n",
        "            #X = np.maximum(0, X)  # Using np.maximum for ReLU activation\n",
        "        elif self.activation_functions[i] == 'softmax':\n",
        "          X = self.softmax(X)\n",
        "\n",
        "        # Store the activation for each layer\n",
        "        self.activations.append(X)\n",
        "\n",
        "      yHat = X\n",
        "      return yHat\n",
        "\n",
        "    def compute_loss(self, yHat, y):\n",
        "      # Cross-entropy loss\n",
        "      m = y.shape[0]\n",
        "      loss = -np.sum(y * np.log(yHat + 1e-9)) / m\n",
        "      return loss\n",
        "\n",
        "\n",
        "    def backwardpass(self, X, y, yHat, learning_rate=0.01):\n",
        "      print(\"shape of X\")\n",
        "      print(X.shape)\n",
        "      print(\"shape of y\")\n",
        "      print(y.shape)\n",
        "      print(\"shape of yHat\")\n",
        "      print(yHat.shape)\n",
        "\n",
        "      \"\"\"\n",
        "      Perform backward propagation through the network and update weights and biases.\n",
        "\n",
        "      Args:\n",
        "      - X (numpy.ndarray): The input data.\n",
        "      - y (numpy.ndarray): The true labels.\n",
        "      - yHat (numpy.ndarray): The predicted output from the forward pass.\n",
        "      - learning_rate (float): The learning rate for updating parameters.\n",
        "\n",
        "      Returns:\n",
        "      - None\n",
        "      \"\"\"\n",
        "      m = y.shape[0]  # Number of examples in the batch\n",
        "      # Store the derivatives for each layer\n",
        "      gradients = []\n",
        "\n",
        "      # Compute the gradient for the output layer (softmax with cross-entropy loss)\n",
        "      dA = yHat - y  # Gradient of the loss with respect to output (yHat)\n",
        "\n",
        "\n",
        "\n",
        "      for i in reversed(range(len(self.layers))):\n",
        "          weights, bias = self.layers[i]\n",
        "          activation = self.activation_functions[i]\n",
        "\n",
        "          # Use the stored activation as the input to this layer\n",
        "          A_prev = self.activations[i]\n",
        "\n",
        "          # Debugging shape of dA and current weights\n",
        "          print(f\"Layer {i + 1}:\")\n",
        "          print(f\"  dA shape: {dA.shape}\")\n",
        "          print(f\"  weights shape: {weights.shape}\")\n",
        "          print(f\"  bias shape: {bias.shape}\")\n",
        "\n",
        "          # Calculate gradients with respect to weights, biases, and inputs for each layer\n",
        "          if activation == 'softmax':\n",
        "              dZ = dA  # dZ for softmax layer\n",
        "          elif activation == 'relu':\n",
        "              dZ = dA * self.relu_derivative(self.activations[i + 1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          # Calculate gradients for weights and biases\n",
        "          dW = np.dot(A_prev.T, dZ) / m\n",
        "          db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "\n",
        "\n",
        "          # Debugging shapes of dW and db\n",
        "          print(f\"Layer {i + 1}:\")\n",
        "          print(f\"  A_prev shape: {A_prev.shape}\")\n",
        "          print(f\"  dA shape: {dA.shape}\")\n",
        "          print(f\"  dZ shape: {dZ.shape}\")\n",
        "          print(f\"  weights shape: {weights.shape}\")\n",
        "          print(f\"  dW shape: {dW.shape}\")\n",
        "          print(f\"  db shape: {db.shape}\")\n",
        "\n",
        "          # Check if shapes align before updating weights\n",
        "          if dW.shape != weights.shape:\n",
        "              raise ValueError(f\"Shape mismatch: dW shape {dW.shape} does not match weights shape {weights.shape}\")\n",
        "          if db.shape != bias.shape:\n",
        "              raise ValueError(f\"Shape mismatch: db shape {db.shape} does not match bias shape {bias.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "          # Update the weights and biases\n",
        "          weights -= learning_rate * dW\n",
        "          bias -= learning_rate * db\n",
        "\n",
        "\n",
        "\n",
        "          # Update the layer in the network with the new weights and biases\n",
        "          self.layers[i] = (weights, bias)\n",
        "\n",
        "          # Update dA for the next layer in the backpropagation process\n",
        "          dA = np.dot(dZ, weights.T)\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward and backward pass, and weight updates for each batch\n",
        "            X_train = X_train.detach().numpy()\n",
        "            X_train= self.flatten(X_train)\n",
        "            yHat = self.feedforward(X_train)\n",
        "            loss = self.compute_loss(yHat, y_train)\n",
        "            print('loss'+loss)\n",
        "            self.backward(X_train, y_train, learning_rate)\n",
        "\n",
        "            # Validation accuracy check (optional)\n",
        "            val_pred = self.forward(X_val)\n",
        "            val_loss = self.compute_loss(val_pred, y_val)\n",
        "            val_accuracy = self.calculate_accuracy(val_pred, y_val)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    def calculate_accuracy(self, yHat, y):\n",
        "        # Calculate accuracy based on predictions and true labels\n",
        "        pred_classes = np.argmax(yHat, axis=1)\n",
        "        true_classes = np.argmax(y, axis=1)\n",
        "        return np.mean(pred_classes == true_classes)\n",
        "\n",
        "    @staticmethod\n",
        "    def hyperparameter_search(X_train, y_train, X_val, y_val, param_grid,input_layers_config, output_layers_config):\n",
        "        best_accuracy = 0\n",
        "        best_params = None\n",
        "\n",
        "        for params in param_grid:\n",
        "            print(f\"Testing configuration: {params}\")\n",
        "            model = NeuralNetwork()\n",
        "            model.addnetwork(input_size=params['input_size'], input_layers_config=input_layers_config, output_layers_config=output_layers_config)\n",
        "\n",
        "\n",
        "            model.train(\n",
        "                X_train, y_train, X_val, y_val,\n",
        "                epochs=params['epochs'],\n",
        "                learning_rate=params['learning_rate']\n",
        "            )\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_pred = model.feedforward(X_val)\n",
        "            val_accuracy = model.calculate_accuracy(val_pred, y_val)\n",
        "\n",
        "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "            # Track the best configuration\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "                best_params = params\n",
        "\n",
        "        print(f\"Best Configuration: {best_params}\")\n",
        "        print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "        return best_params, best_accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3kMrxlMzzgtB"
      },
      "outputs": [],
      "source": [
        "input_layers_config = [\n",
        "    {'neurons': 128, 'activation': 'relu'},\n",
        "    {'neurons': 64, 'activation': 'relu'},\n",
        "    {'neurons': 32, 'activation': 'relu'}\n",
        "]\n",
        "output_layers_config = [\n",
        "    {'neurons': 10, 'activation': 'softmax'}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = F.one_hot(y_train, num_classes=nc)\n",
        "y_test = F.one_hot(y_test, num_classes=nc)"
      ],
      "metadata": {
        "id": "H_2b0sx5IveI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the neural network with flexible layer configuration\n",
        "nn_network = NeuralNetwork()\n",
        "\n",
        "# Convert CNN output to numpy and flatten it\n",
        "cnn_output_np = cnn_output.detach().numpy()\n",
        "cnn_output_np = nn_network.flatten(cnn_output_np)  # Flatten the CNN output\n",
        "print(\"Shape after flattening:\", cnn_output_np.shape)\n",
        "\n",
        "# Add layers to the neural network\n",
        "nn_network.addnetwork(input_size=cnn_output_np.shape[1], input_layers_config=input_layers_config, output_layers_config=output_layers_config)\n",
        "\n",
        "# Training Hyperparameters\n",
        "learning_rate = 0.0001\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Feedforward pass\n",
        "    output = nn_network.feedforward(cnn_output_np)\n",
        "\n",
        "    # Compute Loss (Cross-entropy for classification)\n",
        "\n",
        "    # Ensure labels are in NumPy format\n",
        "    y_train = y_train.detach().cpu().numpy() if isinstance(y_train, torch.Tensor) else y_train\n",
        "\n",
        "    loss = nn_network.compute_loss(output, y_train)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # Backpropagation to update weights\n",
        "    nn_network.backwardpass(cnn_output_np, y_train, output, learning_rate)\n",
        "\n",
        "# Retrieve and display the parameter DataFrame\n",
        "df = nn_network.display_parameters()\n",
        "print(\"Parameter Table for Neural Network:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "7YSoPfM2H9dX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5220f84-d6f4-419b-a2e6-1a6dd5012b47"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 27/100, Loss: 11.6287\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 28/100, Loss: 11.1815\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 29/100, Loss: 11.3346\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 30/100, Loss: 10.3616\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 31/100, Loss: 11.6206\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 32/100, Loss: 11.6837\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 33/100, Loss: 12.1766\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 34/100, Loss: 11.5530\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 35/100, Loss: 12.3044\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 36/100, Loss: 10.0327\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 37/100, Loss: 11.1051\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 38/100, Loss: 11.3330\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 39/100, Loss: 10.0432\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 40/100, Loss: 8.7994\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 41/100, Loss: 8.3040\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 42/100, Loss: 8.4033\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 43/100, Loss: 8.3638\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 44/100, Loss: 8.2922\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 45/100, Loss: 9.5568\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 46/100, Loss: 8.7426\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 47/100, Loss: 8.2504\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 48/100, Loss: 6.0237\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 49/100, Loss: 7.1101\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 50/100, Loss: 7.1034\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 51/100, Loss: 9.2247\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 52/100, Loss: 7.2631\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 53/100, Loss: 5.5170\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 54/100, Loss: 5.8441\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 55/100, Loss: 4.5332\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 56/100, Loss: 5.9942\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 57/100, Loss: 5.5945\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 58/100, Loss: 5.2433\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 59/100, Loss: 5.5024\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 60/100, Loss: 7.2268\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 61/100, Loss: 8.1420\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 62/100, Loss: 7.0141\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 63/100, Loss: 4.7766\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 64/100, Loss: 2.7363\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 65/100, Loss: 3.2552\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 66/100, Loss: 4.0583\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 67/100, Loss: 3.1598\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 68/100, Loss: 4.0441\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 69/100, Loss: 3.2380\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 70/100, Loss: 2.5904\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 71/100, Loss: 3.6358\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 72/100, Loss: 4.1500\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 73/100, Loss: 4.6678\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 74/100, Loss: 3.8988\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 75/100, Loss: 3.1952\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 76/100, Loss: 2.6618\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 77/100, Loss: 3.4449\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 78/100, Loss: 3.5091\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 79/100, Loss: 3.2211\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 80/100, Loss: 2.6539\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 81/100, Loss: 2.0493\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 82/100, Loss: 3.2847\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 83/100, Loss: 2.5409\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 84/100, Loss: 2.2666\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 85/100, Loss: 2.5790\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 86/100, Loss: 2.3635\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 87/100, Loss: 2.8648\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 88/100, Loss: 3.6773\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 89/100, Loss: 3.1798\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 90/100, Loss: 2.9784\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 91/100, Loss: 1.4630\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 92/100, Loss: 1.5502\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 93/100, Loss: 3.0934\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 94/100, Loss: 1.9428\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 95/100, Loss: 2.0257\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 96/100, Loss: 1.8347\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 97/100, Loss: 1.8738\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 98/100, Loss: 1.6190\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 99/100, Loss: 2.9463\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Epoch 100/100, Loss: 3.0793\n",
            "shape of X\n",
            "(64, 2304)\n",
            "shape of y\n",
            "(64, 10)\n",
            "shape of yHat\n",
            "(64, 10)\n",
            "Layer 4:\n",
            "  dA shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  bias shape: (1, 10)\n",
            "Layer 4:\n",
            "  A_prev shape: (64, 32)\n",
            "  dA shape: (64, 10)\n",
            "  dZ shape: (64, 10)\n",
            "  weights shape: (32, 10)\n",
            "  dW shape: (32, 10)\n",
            "  db shape: (1, 10)\n",
            "Layer 3:\n",
            "  dA shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  bias shape: (1, 32)\n",
            "Layer 3:\n",
            "  A_prev shape: (64, 64)\n",
            "  dA shape: (64, 32)\n",
            "  dZ shape: (64, 32)\n",
            "  weights shape: (64, 32)\n",
            "  dW shape: (64, 32)\n",
            "  db shape: (1, 32)\n",
            "Layer 2:\n",
            "  dA shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  bias shape: (1, 64)\n",
            "Layer 2:\n",
            "  A_prev shape: (64, 128)\n",
            "  dA shape: (64, 64)\n",
            "  dZ shape: (64, 64)\n",
            "  weights shape: (128, 64)\n",
            "  dW shape: (128, 64)\n",
            "  db shape: (1, 64)\n",
            "Layer 1:\n",
            "  dA shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  bias shape: (1, 128)\n",
            "Layer 1:\n",
            "  A_prev shape: (64, 2304)\n",
            "  dA shape: (64, 128)\n",
            "  dZ shape: (64, 128)\n",
            "  weights shape: (2304, 128)\n",
            "  dW shape: (2304, 128)\n",
            "  db shape: (1, 128)\n",
            "Parameter Table for Neural Network:\n",
            "            Layer  Input Neurons  Output Neurons  Weights  Biases  \\\n",
            "0     First Layer           2304             128   294912     128   \n",
            "1  Hidden Layer 2            128              64     8192      64   \n",
            "2  Hidden Layer 3             64              32     2048      32   \n",
            "3    Output Layer             32              10      320      10   \n",
            "\n",
            "   Total Parameters  \n",
            "0            295040  \n",
            "1              8256  \n",
            "2              2080  \n",
            "3               330  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loxbFrHnCs3l",
        "outputId": "2ddd9497-b7e6-4c67-aa12-a762e5b8ced6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape after flatteining:- (64, 2304)\n",
            "first layer weights = (2304, 128)\n",
            "first layer bias = (1, 128)\n",
            "Layer 1:\n",
            "  Input X shape: (64, 2304)\n",
            "  Weights shape: (2304, 128)\n",
            "  Bias shape: (1, 128)\n",
            "Layer 2:\n",
            "  Input X shape: (64, 128)\n",
            "  Weights shape: (128, 64)\n",
            "  Bias shape: (1, 64)\n",
            "Layer 3:\n",
            "  Input X shape: (64, 64)\n",
            "  Weights shape: (64, 32)\n",
            "  Bias shape: (1, 32)\n",
            "Layer 4:\n",
            "  Input X shape: (64, 32)\n",
            "  Weights shape: (32, 10)\n",
            "  Bias shape: (1, 10)\n",
            "Parameter Table for Neural Network:\n",
            "            Layer  Input Neurons  Output Neurons  Weights  Biases  \\\n",
            "0     First Layer           2304             128   294912     128   \n",
            "1  Hidden Layer 2            128              64     8192      64   \n",
            "2  Hidden Layer 3             64              32     2048      32   \n",
            "3    Output Layer             32              10      320      10   \n",
            "\n",
            "   Total Parameters  \n",
            "0            295040  \n",
            "1              8256  \n",
            "2              2080  \n",
            "3               330  \n"
          ]
        }
      ],
      "source": [
        "#Initialize the neural network with flexible layer configuration\n",
        "nn_network = NeuralNetwork()\n",
        "cnn_output_np = cnn_output.detach().numpy()\n",
        "cnn_output_np=nn_network.flatten(cnn_output_np)  # Flatten the CNN output\n",
        "print(\"shape after flatteining:-\", cnn_output_np.shape)\n",
        "nn_network.addnetwork(input_size=cnn_output_np.shape[1], input_layers_config=input_layers_config, output_layers_config=output_layers_config)\n",
        "\n",
        "output = nn_network.feedforward(cnn_output_np)\n",
        "\n",
        "# Retrieve and display the parameter DataFrame\n",
        "df = nn_network.display_parameters()\n",
        "print(\"Parameter Table for Neural Network:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_NiTISulOzV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Convert validation set to numpy arrays for easy handling with the NeuralNetwork class\n",
        "# Using DataLoader for batch processing if needed\n",
        "val_loader = DataLoader(new_validation_dataset, batch_size=len(new_validation_dataset))\n",
        "X_val, y_val = next(iter(val_loader))  # Get entire validation set in one batch\n",
        "X_val = X_val.view(len(X_val), -1).numpy()  # Flatten and convert to numpy array\n",
        "y_val = torch.nn.functional.one_hot(y_val, num_classes=10).numpy()  # One-hot encode labels\n",
        "\n",
        "# Define parameter grid for hyperparameter search\n",
        "param_grid = [\n",
        "    {'input_size': X_val.shape[1], 'epochs': 10, 'learning_rate': 0.01},\n",
        "    {'input_size': X_val.shape[1], 'epochs': 20, 'learning_rate': 0.001},\n",
        "    {'input_size': X_val.shape[1], 'epochs': 30, 'learning_rate': 0.005}\n",
        "]\n",
        "\n",
        "# Perform hyperparameter search\n",
        "best_params, best_accuracy = NeuralNetwork.hyperparameter_search(new_train_dataset, new_test_dataset, X_val, y_val, param_grid, input_layers_config, output_layers_config)\n",
        "\n",
        "print(f\"Best hyperparameters: {best_params}\")\n",
        "print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}